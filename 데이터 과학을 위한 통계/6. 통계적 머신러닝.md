# 6. 통계적 머신러닝

## 6.1 k-최근접 이웃 (KNN)
- 특징(예측변수)들이 가장 유사한 k개의 레코드를 찾음 → 유사한 레코드 중 '(분류) 다수가 속한 class', '(예측) 평균'을 예측값으로 사용
	- 분류 문제에서 가까운 점들이 속한 class의 비율을 통해 확률 계산 가능
		- 불균형 문제를 다룰 때 컷오프 확률을 정하여 활용할 수 있음 
	- 모든 예측 변수들은 수치형이여야함 (범주형은 가변수화해서 사용)
- 유사성(거리)
	- 유사성은 거리 지표를 통해 결정 (서로 얼마나 멀리 떨어져 있는지 측정)
	1. 유클리드 거리
		- 두 점 사이의 직선(대각선) 거리
		- 두 데이터 사이의 차이에 대한 제곱합을 구한뒤 그 값의 제곱근을 취함
		- 쌍대비교가 필요해 데이터 개수가 많아질 수록 계산량이 증가함
	2. 맨해튼 거리
		- 대각선이 아닌, 한 축 방향으로만 움직일 수 있을 때의 거리
		- 점과 점 사이의 이동시간으로 근접성을 따질 때 좋은 지표
	3. 마할라노비스 거리
		- 두 변수 사이의 상관관계 사용 (높은 상관관계 있을 때 유용)
		- 많은 계산이 필요하고 복잡성이 증가하는 단점이 있음
- 데이터 표준화
	- 거리 측정 시 data scale에 큰 영향을 받을 수 있어 데이터 표준화(정규화) 필요
		- 변수에서 평균을 빼고 표준편차로 나눠서 변환
		- 변환 결과인 z-score는 평균으로부터 표준편차만큼 얼마나 떨어져 있는지를 의미함
	- 다른 표준화 방법 사용 가능(평균 대신 중간값 사용, 표준편차 대신 사분위범위 사용 등)
	- 일부 변수가 다른 변수보다 중요하다는 주관적 지식이 있는 경우 모든 변수의 중요성이 같아지는 표준화를 적용하지 않도록 주의
	- 정규화는 데이터 분포에 영향을 주지 않음!
- k 개수 선택
	- 데이터의 노이즈가 적으면 k 값이 작을수록 잘 동작함 
		- 보통의 데이터일 경우 k가 너무 작으면 노이즈까지 고려하는 오버피팅 문제 발생
		- 너무 크면 과하게 평탄화되어 지역정보 예측의 이점을 가진 knn의 기능을 잃어버릴 수도
	- 홀드아웃 세트를 활용해 성능지표(ex. 정확도) 비교로 k 값 결정
	- 일반적으로 k를 1 ~ 20 사이의 값으로 설정하며, 동률이 나오는 것을 방지하기 위해 홀수 사용
	
> 실제 활용 (앙상블/피처엔지니어링)
- 다른 복잡한 분류 방법에 비해 경쟁력이 낮지만, 다른 분류 방법들의 중간 단계에서 '지역적 정보 (예측변수)'를 추가하는 데 활용
	- knn으로 class에 속할 확률을 도출 → 이 결과를 새로운 특징(feature)으로 추가 → 다른 분류 모델에 해당 변수 사용
	```
	ex. 최근에 팔린 비슷한 집들의 가격을 새로운 특징으로 추가
	    → 각 레코드별 knn 예측값(k-최근접 이웃값의 평균)을 사용
	```