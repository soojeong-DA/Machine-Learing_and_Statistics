# 4. 회귀와 예측
```
통계학의 일반적인 목표
- 변수 X(X1, X2, ...)가 변수 Y와 관련이 있는가?
- 있다면 어떤 관련이 있는가?
- 이를 이용해 Y를 예측할 수 있는가?
```

## 4.1 단순선형회귀
- 한 변수와 다른 변수의 크기 사이에 어떤 관계를 정량화하는 방법/모델
	- X가 얼마큼 변하면 Y가 어느 정도 변하는지 추정할 수 있음
	- 예측과 설명(관계 이해)에 사용됨
	```
	ex. y= 424.583 -4.185x1  → x가 1씩 증가할 때마다 y는 -4.185의 비율로 줄어듬
	```
- 최소자승법(OLS)
	- 잔차제곱합(RSS)를 최소화하는 값을 구하는 방법
	- 빠르고 쉽게 계산 가능
	- 특잇값에 매우 민감
- 회귀방정식 자체가 인과관계를 정확히 증명하는 것이 아님 → 해당 도메인 지식 함께 고려해 결론 내려야

## 4.2 다중선형회귀
- 한 응답변수와 여러개의 예측변수간의 관계
	- 다른 모든 변수가 고정되었다고 가정했을 때, 한 변수가 변하는 정도에 따라 예측값도 계수에 비례해 변화함

- 모형 평가 (in sample)
	1. RMSE(Root Mean Squared Error)
		- 예측된 값들에 대한 평균제곱오차의 제곱근
		- 회귀모형을 평가하는데 가장 많이 사용되는 측정지표
	2. RSE(Residual Standard Error)
		- 평균제곱오차와 동일하지만, 자유도에 따라 보정된 값
		- 예측변수의 개수(p) 고려
	→ 빅데이터에서는 RMSE와 RSE 차이 작음
	3. R-squared(결정계수)
		- 모델에 의해 설명된 분산의 비율 (데이터의 변동률)
		- 값 범위 0 ~ 1
	4. adjusted R-squared
		- 모델에 더 많은 예측변수를 추가하는 것에 효과적으로 페널티를 가함
	→ 빅데이터에서는 일반 R제곱과 수정된 R제곱의 차이 작음
	5. t-statistic
		- 모델에서 변수의 중요도를 비교하는 기준이됨 (변수 선택에 유용하게 사용됨)
		- t통계량이 높을 수록(p-value가 낮을수록) 예측변수는 더 유의미함

- k-fold 교차타당성검사 (out of sample)
	- 홀드아웃 데이터를 사용하여 

- 모형 선택 (in sample)
	- `많은 변수를 추가한다고 꼭 더 좋은 모델을 만드는 것이 아님!`
		- 변수를 추가할수록 항상 RMSE는 감소하고, R-squared은 증가
		- 동일한 조건이라면 단순한 모델을 우선 사용
	-  AIC를 낮추는 or R-squared를 높이는 방향으로 변수/모형 선택
		- AIC는 모델에 항을 추가할수록 불이익을 주는 측정 지표
	1. 전진선택/후진제거
		- 전진선택
			- 예측변수 없이 시작하여, 각 단계에서 R-squared에 가장 큰 기여도를 갖는 예측변수를 하나씩 추가
			- 기여도가 통계적으로 더는 유의미하지 않을 때 중지
		- 후진제거
			- 전체 모델로 시작해서, 통계적으로 유의하지 않은 예측 변수들을 제거해나감
			- 모든 예측변수가 통계적으로 유의미한 모델이 될 때까지 계속됨
	2. 벌점회귀 (penalized regression)
		- 변수(파라미터)에 대해 모델에 불이익을 주는 제약조건을 추가
		- 변수를 완전히 제거하지 않고, 계수의 크기를 감소시키거나 거의 0으로 만들어 패널티 적용 ex. lasso
- 가중회귀
	- 레코드별로 가중치를 주기 위해 사용
		- ```
			ex. 오래된 매매 정보는 최근 정보보다는 신뢰하기 어려움 
		        → 정보 날짜의 년도와 현재와의 차이를 가중치로 사용
		  ```

## 4.3 회귀를 이용한 예측
- 외삽
	- 모델링에 사용된 데이터 범위를 벗어난 부분까지 모델을 확장하는 것
	- 데이터 범위를 벗어나는 외삽은 오류를 유발할 수 있음
		-ex. 시계열 예측을 위해 회귀를 고려하지 않음
- 신뢰구간
	- 회귀계수 주변의 불확실성을 정량화
	- 여러 값에서 계산된 평균이나 다른 통계량과 관련
- 예측구간
	- '개별' 예측값의 불확실성을 정량화
	- 일반적인 데이터 과학에서는 개별 예측에 관심이 있으므로 예측구간이 더 적절할 수도 있음

## 4.4 회귀에서의 요인변수(범주형 변수)
- 모델에 사용할 수 있도록 수치형 변수로 변환 필요
- 다수의 수준을 갖는 요인변수의 경우 더 적은 수의 수준을 갖는 변수가 되도록 수준 통합 필요
	- 변수와 결과 간의 관계를 탐색하여 '모든 요소 유지' or '수준 통합(범주/그룹화)' 결정해야
	- 다른 변수에 따라 그룹으로 묶거나, 초기 모델의 잔차를 사용하여 그룹화하는 방법이 있음
1. 일반적인 요인변수 변환
	1. 더미변수(가변수)
		- 0, 1 이진변수로 부호화
	2. 기준 부호화
		- 한 요인을 기준으로 하고, 다른 요인들이 이 기준에 따라 비교할 수 있도록한 것 (상대적인 값으로)
	3. 원-핫 인코딩
		- 머신러닝에서 많이 사용되는 부호화
		- 모든 요인 수준이 계속 유지됨 → 회귀에서는 다중공선성 유발할 수 있어 적합하지 않음
		- drop_first=True로 설정해 다중공선성 문제를 피하는 변환 가능 (P-1개, 기준 부호화랑 똑같아짐)
	4. 편차 부호화
		- 전체 평균에 대해 각 수준을 비교하는 부호화 방법
2. 순서가 있는 요인변수 변환
	- 숫자 값으로 변환하여 그대로 사용 (순서 정보 유지하기 위해)

## 4.5 회귀방정식 해석 시 주의
- 변수간 상관
	- 상호 연관이 있는 예측변수들을 사용하면 회귀계수의 부호와 값의 의미를 해석하기 어려울 수 있음 (추정치의 표준오차가 커짐)
- 다중공선성
	- 변수 상관이 극단적인 경우 다중공선성이 나타남 (예측변수 사이의 중복성)
		- ex. 오류로 한 변수가 여러번 포함, P-1개가 아닌 P개의 가변수가 만들어진 경우, 두 변수가 거의 완벽하게 상관성이 있는 경우
	- 회귀분석 시 다중공선성이 사라질 때까지 변수를 제거해야함
	- 트리, 클러스터링, 최근접 이웃 알고리즘 등 비선형회귀 유형의 방법에서는 큰 문제가 되지는 않음
		- P개의 가변수를 유지하는 것이 좋음
- 교란변수
	- 생략된 중요한 예측변수
	- 이 경우 방정식 계수에 대한 해석이 잘못된 결론으로 이어질 수 있음

## 4.6 회귀 진단
- 특잇값
	- 회귀에서 특잇값은 실제 y값이 예측된 값에서 멀리 떨어져 있는 경우를 말함
		- 잔차를 표준오차로 나눈 표준화잔차로 특잇값 검출에 사용
	- 데이터 크기가 클 경우, 특잇값이 큰 문제가 되지는 않음
- 영향값
	- 회귀모형에서 제외됐을 때 모델에 중요한 변화를 가져오는 값
	- 단일 레코드의 영향력 지표
		- 햇 값, 쿡의 거리
		- 영향력/거품 그림은 표준화잔차, 햇 값, 쿡의 거리를 모두 한 그림에 표현함
	- 데이터 크기가 클 경우, 어떤 한 값이 회귀식에 큰 변화를 주지는 않음