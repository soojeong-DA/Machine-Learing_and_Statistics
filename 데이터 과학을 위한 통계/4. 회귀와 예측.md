# 4. 회귀와 예측
```
통계학의 일반적인 목표
- 변수 X(X1, X2, ...)가 변수 Y와 관련이 있는가?
- 있다면 어떤 관련이 있는가?
- 이를 이용해 Y를 예측할 수 있는가?
```

## 4.1 단순선형회귀
- 한 변수와 다른 변수의 크기 사이에 어떤 관계를 정량화하는 방법/모델
	- X가 얼마큼 변하면 Y가 어느 정도 변하는지 추정할 수 있음
	- 예측과 설명(관계 이해)에 사용됨
	```
	ex. y= 424.583 -4.185x1  → x가 1씩 증가할 때마다 y는 -4.185의 비율로 줄어듬
	```
- 최소자승법(OLS)
	- 잔차제곱합(RSS)를 최소화하는 값을 구하는 방법
	- 빠르고 쉽게 계산 가능
	- 특잇값에 매우 민감
- 회귀방정식 자체가 인과관계를 정확히 증명하는 것이 아님 → 해당 도메인 지식 함께 고려해 결론 내려야

## 4.2 다중선형회귀
- 한 응답변수와 여러개의 예측변수간의 관계
	- 다른 모든 변수가 고정되었다고 가정했을 때, 한 변수가 변하는 정도에 따라 예측값도 계수에 비례해 변화함

- 모형 평가 (in sample)
	1. RMSE(Root Mean Squared Error)
		- 예측된 값들에 대한 평균제곱오차의 제곱근
		- 회귀모형을 평가하는데 가장 많이 사용되는 측정지표
	2. RSE(Residual Standard Error)
		- 평균제곱오차와 동일하지만, 자유도에 따라 보정된 값
		- 예측변수의 개수(p) 고려
	→ 빅데이터에서는 RMSE와 RSE 차이 작음
	3. R-squared(결정계수)
		- 모델에 의해 설명된 분산의 비율 (데이터의 변동률)
		- 값 범위 0 ~ 1
	4. adjusted R-squared
		- 모델에 더 많은 예측변수를 추가하는 것에 효과적으로 페널티를 가함
	→ 빅데이터에서는 일반 R제곱과 수정된 R제곱의 차이 작음
	5. t-statistic
		- 모델에서 변수의 중요도를 비교하는 기준이됨 (변수 선택에 유용하게 사용됨)
		- t통계량이 높을 수록(p-value가 낮을수록) 예측변수는 더 유의미함

- k-fold 교차타당성검사 (out of sample)
	- 홀드아웃 데이터를 사용하여 

- 모형 선택 (in sample)
	- `많은 변수를 추가한다고 꼭 더 좋은 모델을 만드는 것이 아님!`
		- 변수를 추가할수록 항상 RMSE는 감소하고, R-squared은 증가
		- 동일한 조건이라면 단순한 모델을 우선 사용
	-  AIC를 낮추는 or R-squared를 높이는 방향으로 변수/모형 선택
		- AIC는 모델에 항을 추가할수록 불이익을 주는 측정 지표
	1. 전진선택/후진제거
		- 전진선택
			- 예측변수 없이 시작하여, 각 단계에서 R-squared에 가장 큰 기여도를 갖는 예측변수를 하나씩 추가
			- 기여도가 통계적으로 더는 유의미하지 않을 때 중지
		- 후진제거
			- 전체 모델로 시작해서, 통계적으로 유의하지 않은 예측 변수들을 제거해나감
			- 모든 예측변수가 통계적으로 유의미한 모델이 될 때까지 계속됨
	2. 벌점회귀 (penalized regression)
		- 변수(파라미터)에 대해 모델에 불이익을 주는 제약조건을 추가
		- 변수를 완전히 제거하지 않고, 계수의 크기를 감소시키거나 거의 0으로 만들어 패널티 적용 ex. lasso
- 가중회귀
	- 레코드별로 가중치를 주기 위해 사용
		- `ex. 오래된 매매 정보는 최근 정보보다는 신뢰하기 어려움 
		        → 정보 날짜의 년도와 현재와의 차이를 가중치로 사용`

## 4.3 회귀를 이용한 예측
- 외삽
	- 모델링에 사용된 데이터 범위를 벗어난 부분까지 모델을 확장하는 것
	- 데이터 범위를 벗어나는 외삽은 오류를 유발할 수 있음
		-ex. 시계열 예측을 위해 회귀를 고려하지 않음
- 신뢰구간
	- 회귀계수 주변의 불확실성을 정량화
	- 여러 값에서 계산된 평균이나 다른 통계량과 관련
- 예측구간
	- '개별' 예측값의 불확실성을 정량화
	- 일반적인 데이터 과학에서는 개별 예측에 관심이 있으므로 예측구간이 더 적절할 수도 있음
