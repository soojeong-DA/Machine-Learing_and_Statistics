# 5. 분류
```
- 이진분류(0 or 1), 세가지 이상의 class 중 어디에 속할지 예측
	- 대부분의 알고리즘은 관심 클래스에 속할 확률을 반환함
	- 관심 class에 대한 컷오프(절사) 확률을 정하여 조정 가능
		- 컷오프가 높을수록 1(관심 class)로 예측되는 레코드가 적어짐
		- 컷오프가 낮을수록 더 많은 레코드가 1로 예측됨
- 하나의 카테고리가 다른 카테고리보다 훨씬 더 일반적인 경우
	- 멀티 클래스 문제라도 이진 분류 문제로 변환하는 것이 유리할 수도 있음
```

## 5.1 나이브 베이즈
- 주어진 결과에 대해 예측변수값을 관찰할 확률(사전 확률)을 사용하여, 예측변수가 주어졌을 때 결과를 관찰할 확률(사후 확률)을 반환
 	- 예측변수들이 서로 독립이라고 가정하는 베이즈 정리 이용 (모든 사건이 독립사건이라는 naive한 가정)
	- 모델로부터 나온 결과가 조건부확률이 됨
		- 조건부확률
			- 어떤 사건이 주어졌을 때, 해당 사건을 관찰할 확률
		- 사후확률
			- 예측 정보를 통합한 후 결과의 확률
			- 사전확률에서는 예측변수에 대한 정보를 고려하지 않음
		- ```
		    "각 출력 카테고리 안에서, 어떤 예측변수의 카테고리가 가장 가능성이 높은가?"의 확률을 출력
		    → 이 정보는 주어진 예측변수 값에 대해 결과 카테고리의 확률을 추정하는 것으로 바뀜 (조건부확률로 활용됨)
		  ```
- 예측변수가 범주형인 경우에 적합하며, 수치형인 경우 구간화하여 범주형으로 변환해야함

## 5.2 판별분석
- 선형판별분석(LDA, Linear discriminant analysis)
	- 가장 일반적으로 사용되는 판별분석 기법
	- 트리 모델, 로지스틱 회귀 같은 더 정교한 기법 출현 이후 LDA는 많이 사용하지 않고, 일부 응용 분야에서만 사용됨
	- 예측변수가 정규분포를 따르는 연속적인 변수여야하는 이론적인 가정이 있지만, 정규분포에서 벗어나거나 이진 예측변수에도 잘 동작함
	- '사이 제곱합(다른 그룹 간의 편차 제곱) / 내부 제곱합(그룹 안의 편차)'의 비율을 최대화하는 선형결합을 찾음
		- 사이 제곱합을 최대화, 내부 제곱합을 최소화하는 것이 분류를 가장 명확히 하는 방법
	- 변수 선택에 사용 가능
		- 예측변수들을 정규화한 뒤 LDA를 돌린 결과인 '판별 가중치' = '변수의 중요도'
	- 예측결과인 확률(가중치)를 시각화해서 모델이 잘 동작하는 지 판단 가능
- 이차판별분석(QDA, Quadratic discniminant analysis)
	- 선형판별함수를 사용하지만 가정이 다름
		- LDA: Y=0, Y=1 두 그룹 간의 공분산행렬이 모두 동일해야함
		- QDA: 두 그룹이 서로 다른 공분산을 가질 수 있음

→ 실무에서 대부분의 경우 두 판별분석 기법의 차이가 크지 않음 

## 5.3 로지스틱 회귀
- 결과가 이진형 변수라는 것만 빼면 선형회귀와 유사
	- 로지스틱 회귀는 최대우도추정(MLE) 사용함
		- 응답변수는 0 or 1이 아닌, 응답이 1인 로그 오즈비의 추정치임
			- 오즈(odds): 실패(0)에 대한 성공(1)의 비율
		- 편차 지표를 사용하여 모델을 평가
	- 선형회귀에서 처럼 범주형(요인)변수 인코딩해야함 (기준인코딩, 원핫인코딩)
- 데이터 기반의 접근방식(ex. knn, naive bayes)보다는 구조화된 모델 접근 방식
- 계산 속도가 빠르며, 새로운 데이터에 대해서도 간단한 산술연산으로 빠르게 결과를 구할 수 있음 ← 많이 사용되는 이유
- 파이썬에서 로지스틱 회귀
	- L1 or L2 정규화에 의한 과적합을 방지하기 위해 penalty와 C 인수 사용 (기본으로 정규화는 적용되어 있음)
	- solver 인수는 사용할 최소화 방법 선택 (기본값 = liblinear)

> p-value
- R로 로지스틱 회귀 모델링 했을 때 p-value 해석
	- 통계적인 유의성 측정 지표로 보기 보다는 '변수의 중요성'을 나타내는 상대적인 지표로 봐야함