# 2. 데이터와 표본분포
✔	
```
빅데이터를 효과적으로 다루고, 편향을 최소화하려면 표본추출이 중요
	- 빅데이터 프로젝트도 결국에는 표본/샘플을 활용해 테스트 or 예측 모델 개발함
```

## 2-1. 임의표본추출과 표본편향
1. 표본 추출
	1. 임의/랜덤표본추출
		- 무작위로 표본 추출
		- 접근 가능한 모집단을 적절하게 정의하는 것이 중요
			- 조사 대상, 표본추출 절차
	2. 층화표본추출
		- 모집단을 여러 층으로 나눈 뒤, 각 층(strata)에서 무작위로 표본 추출
			- 각 층은 공통된 특징을 가진 그룹
		- 계층마다 동일한 표본크기를 얻을 수 있다는 장점
			- 여러 그룹 중 특정 그룹의 수가 적은 경우, 해당 층에 높은 가중치를 주는 표본추출 활용

2. 편향
	1. 편향
		- 측정/관측 과정 or 표본추출 과정에서 발생하는 계통적인(systematic) 오차
		- 임의표본추출로 인한 오류 vs 편향에 따른 오류 **구분!**
			- 임의표본추출로 인한 오류는 랜덤함 (어느 쪽으로 강하게 치우치는 경향 없음)
			- 편향이 있는 오류는 특정 지점에 강하게 치우쳐 나타나는 경햠이 있음 
	2. 표본편향
		- 모집단을 제대로 대표하지 못하는 표본
		- 모집단과 표본 사이의 차이가 유의미할 만큼 크고, 동일한 방식으로 추출된 다른 샘플들에도 이 차이가 계속될 것으로 예상될 때 표본편향이 발생했다고 볼 수 있음


## 2-2. 선택편향
```
"도출된 결과가 정말로 의미있는 것일까?"
→ 단순 우연히 얻은 예외의 경우일 수도 있음을 주의
→ 가설을 구체적으로 명시하고, 임의표본추출 원칙에 따라 데이터를 수집하면 편향을 피할 수 있음
```
- 선택편향
	- 의식적/무의식적으로 데이터를 선택적으로 골라 발생하는 편향
	- 결국 오해의 소지가 있거나, 단변적인 결론을 얻게됨
- 데이터 스누핑
	- 흥미로운 것을 찾아 광범위하게 데이터를 살피는 것 (파다보면 뭐라도 나올 것이다 🤷‍♀️)
- 방대한 검색 효과
	- 중복 데이터 모델링이나 너무 많은 예측변수를 고려하는 모델링에서 발생하는 편향 or 비재현성
- 평균으로의 회귀
	- 어떤 변수를 연속적으로 측정 시 나타나는 현상
	- 예외적인 경우가 관찰되면, 그 다음에는 중간 정도의 경우가 관찰되는 경향이 있음
		- 예외 경우를 너무 특별하게 생각하고 의미를 부여하면, 선택편향으로 이어질 수 있음

## 2-3. 통계학에서의 표본분포
``` 
주요 관심사는 '표본의 변동성' 
- 표본에 따라 결과가 얼마나 달라질지
- 중심극한정리에 의존하는 공식 or 부트스트랩 방식을 통해 '표본분포' 추정 가능
```
- 표본분포   != 개별 데이터 분포
	- **표본통계량**의 분포 ex. 평균, 분산, ...
	- 데이터 자체의 분포보다 규칙적이고 종 모양일 가능성이 높음 
		- 표본이 클수록 가능성이 높으며, 분포가 좁아짐
- 중심극한정리
	- 표본크기가 커질수록 표본분포(평균)가 정규분포(종모양의 정규곡선)를 따르는 경향
		- 일반적으로는 n >= 30

→ 하지만 데이터 과학의 관점에서는 **부트스트랩**을 사용할 수 있어, 중심극한 정리가 크게 중요하지 않음

- 표준오차(standard error)
	- 여러 표본들로부터 얻은 `표본통계량`의 변량  != 표준편차 (개별 데이터 `값`들의 변량, s)
		- 표본분포/표본통계량의 변동성을 요약하는 지표
	- SE = s / sqrt(n)
	- 표본크기가 커지면 표준오차가 줄어듬
	- 표준오차와 표본크기 사이의 관계 'n 제곱근의 법칙'
		- 표준오차를 2배로 줄이려면, 표본크기를 4배 증가시켜야

## 2-4. 부트스트랩 ✨
- 통계량/표준오차, 모델 파라미터의 표본분포를 추정하는 쉽고 효과적인 방법
	- 모든 통계에 사용할 수 있으며, 중심극한정리 or 기타 분포 가정에 의존하지 않음!!
- 현재 있는 표본에서 추가적으로 표본을 `복원추출`하고, 각 표본에 대한 통계량과 모델을 **다시 계산**하는 방법
	- 뽑을 때마다 각 원소가 뽑힐 확률을 그대로 유지하면서, 무한한 크기의 모집단을 만들어낼 수 있음!	
	- 반복 횟수가 많을 수록, 표준오차나 신뢰구간에 대한 추정이 더 정확해짐      
    
→ 결과적으로 표본통계량, 추정한 모델 파라미터의 부트스트랩 집합을 얻게되고, 이 집합의 `변동성(안정성)`을 평가할 수 있음      
❌ 표본크기가 작은 것을 보완하는 게 아님, 새 데이터를 만드는 것도 아님, 기존 데이터의 빈 곳을 채는 것도 아님!

> 재표본추출 vs 부트스트랩
- 재표본추출 여러 표본이 결합되어 비복원추출 or 복원추출을 수행할 수 있는 순열(셔플링)과정을 포함함
- 부트스트랩은 항상 복원 추출

## 2-5. 신뢰구간
- 점추정(단일 수치)가 아닌, 구간 범위로 추정값을 제시
	- 신뢰수준이 높을수록, 표본이 작을수록 구간이 넓어짐(불확실성 커짐)
- 신뢰수준
	- 같은 모집단에서 같은 방식으로 얻은, 관심 통계량을 포함할 것으로 예상되는, 신뢰구간의 백분율 (90%, 95%)

> 실제 데이터 과학에서의 신뢰구간 용도
- 표본 결과가 얼마다 달라질 수 있는지 (변동성)
- 추정에 대한 잠재적인 오류 파악할 때
- 더 큰 표본이 필요한지 여부를 파악할 때
- ❌논문 발표나 결과 보고에 신뢰구간을 사용하지는 않음

## 2-6. 정규분포
- 전통적인 통계의 상징이며, **과대평가**된 측면이 있음
	- 실제 대부분의 원시 데이터는 전반적으로 정규분포를 따르지 않음
	- 표본들의 평균, 합계, 오차 등은 대부분 정규분포를 따름
- 정규분포 가정은 경험적 확률분포나 부트스트랩 분포를 구할 수 없는 경우 사용되는 **최후의 수단** 

- **표준정규분포(Z 분포)**
	- 평균=0, 표준편차=1인 정규분포
	- x축의 단위가 평균의 표준편차로 표현됨
	- 비교 편리

- QQ-plot
	- 표본분포가 특정 분포(ex. 정규분포)에 얼마나 가까운지를 시각적으로 판별하는데 사용됨
	- 점들이 대략 대각선 위에 높이면, 표본분포가 정규분포에 가까운 것으로 간주
		- y축: z-score, x축: 정규분포의 분위수

> 표준화/정규화: 평균을 빼고 표준편차로 나눔

> Z-score: 개별 데이터 포인트를 정규화한 값

- 데이터를 z-score로 변환한다고 해서, 데이터가 정규분포가 되는게 아님!
- just **비교 목적**으로 데이터를 표준정규분포와 **같은 척도**로 만드는 것뿐

## 2-7. 긴 꼬리 분포
- 실무에서는 긴 꼬리를 잘 들여다보는 것을 중요하게 여김
- 흑고니 이론 (black swan theory)
	- 어떤 이례적인 현상(극단값)이 정규분포로 예측되는 것보다 훨씬 더 자주 일어날 수 있다고 예측하는 것


## 2-8. 스튜던트의 t 분포
- 정규분포와 비슷한 모양이지만, 꼬리 부분이 약간 더 길고 두꺼움
	- 표본이 클수록 더 정규분포를 닮아감
- 표본통계량의 분포를 설명하는 데 광범위하게 사용됨
 	- ex. 표본평균, 두 표본평균 간의 차이, 회귀 파라미터

→ 컴퓨터가 없던 시절 표본통계량의 보통 정규분포를 따르는 경향이 있어, t 분포가 널리 사용됐었음
→ 지금은 불확실성과 변동성을 정량화 하기 위한 방법으로 **부트스트랩** 사용

## 2-9. 이항분포 (베르누이 분포)
- 각 시행은 정해진 확률로 **두 가지** 결과를 가짐 (이항시행, 베루누이 시행)
- 각 시행마다 성공 확률(p)이 정해져 있을 때, 주어진 시행 횟수(n) 중에서 **성공한 횟수(x)**의 도수분포
	- 성공: 시행에서 관심 있는 결과
	- 시행: 독립된 결과를 가져오는 하나의 사건 ex. 동전 던지기
	- 평균 = n*p   → 예상되는 성공 횟수로 생각할 수도 있음
	- 분산 = n*p(1-p)
	```python
	# ex1. 한 번의 클릭이 판매로 이어질 확률이 0.02일 때, 200회 클릭으로 0회 매출을 관찰할 확률은?
	# 각 시행의 성공 확률 p = 0.02, size = 200, x=0
	stats.binom.pmf(0, n=200, p=0.02)  # 0.0176
	```
- 보통은 n번의 시도에서 **x번 또는 그 이하로 성공할 확률이 얼마인지** 알아보는 데 관심이 있음
	```python 
	stats.binom.cdf(2, n=5, p=0.1)
	```
- 시행 횟수가 충분한 경우 (특히 p=0.5에 가까울 때) 이항분포는 정규분포와 거의 비슷해짐
	- 표본크기가 커질수록 이항 확률을 구하기 위해 많은 계산이 필요해, 평균과 분산으로 근사화한 정규분포 사용함
		
## 2-10. 카이제곱분포
- 범주에 속하는 주제 or 항목의 `수(횟수)`와 관련이 있음
- 카이제곱검정
	- 카이제곱통계량
		- 독립성에 대한 귀무 가설의 기댓값에서 벗어난 정도를 측정
			- 귀무가설, 기댓값: 데이터에서 특이하거나 주목할 만한 것이 없음, 관계가 없음
			- 카이제곱 값이 높다면, 기대한 것과 현저히 다름을 나타냄
	1. 독립성 검정
		- 변인이 **두 개 이상**일 때 사용
		- 관찰된 빈도가 기대 빈도와 `의미있게 다른지의 여부`를 검정
			- `기대빈도`: '두 변인이 서로 **상관이 없고 독립적**'이라고 기대하는 것
 		- 여러 처리**(A/B/C ... 검정)의 효과가 서로 다른지 여부**를 결정하는 데 유용 ✔
	2. 적합도 검정
		- 관측 데이터가 특정 분포(ex. 이항분포, 정규분포, ..)에 적합한 정도를 나타냄

## 2-11. F 분포
- 여러 그룹의 서로 다른 처리 테스트를 검정하는 데 사용나, ~~횟수(카이제곱분포)~~가 아닌, `연속된 관측값`을 다룸
- F 통계량
	- 그룹간의 평균 차이가 정규 무작위 변동에서 예상할 수 있는 것보다 얼마나 큰지 측정
	- 그룹 평균 간 변동성 / 각 그룹 내 변동성 ← ANOVA
	- 실험 및 선형 모델에 사용됨
		- 선형 모델: 모델/모형에 의해 설명된 변동성을 데이터 전체의 변동과 비교

## 2-12. 푸아송 분포, 지수분포, 베이불 분포
- lambda가 `해당 기간동안 일정하게 유지`된다는 가정을 충족할 때 푸아송 분포, 지수분포 사용 가능
	- 실제 해당 가정이 적절하지 않은 경우가 많지만
	- 일정기간 충분히 동일하도록 시간 주기 or 공간의 영역을 잘 나눈다면(세분화), 해당 기간 내의 분석 및 시뮬레이션이 가능
1. 푸아송 분포
	- 표집된 `단위 시간 or 단위 공간`에서 발생한 사건의 도수분포
		- 람다(lambda): 어떤 일정 `시간/공간의 구간 안`에서 발생한 **평균 사건 수**
	- 시간 주기별 이벤트를 모델링 하는 데 가장 적합한 분포 (인터넷 트래픽 수준에 대한 데이터)		
	- 대기행렬 (시뮬레이션) 관련 문제를 해결할 때 유용
		```python
		# ex. 고객 서비스 센터에서 1분당 평균 2회로 문의 전화가 접수된다면, 100분당 문의 전화 횟수는?
		# lambda = 2
		stats.poisson.rvs(2, size=100) 
		```
2. 지수 분포
	- 동일한 변수 lambda를 사용
	- 한 사건과 다음 사건 간의 시간 분포 모델링 가능  (사이 시간)
		- ex. 고장이 발생하는 시간, 개별 고객 상담에 소요되는 시간

→ 다수의 경우 사건 발생률은 시간에 따라 일정하지 않음

3. 베이불 분포
- 사건 발생률이 시간에 따라 `지속적으로 변할 때` 유용 
	- 베타 > 1 : 발생률은 시간이 지남에 따라 증가
	- 베타 < 1 : 발생률이 시간이 지남에 따라 감소
- 발생률 대신 고장 시간 분석에도 사용됨으로, **특성 수명**이 인수로 사용됨 
